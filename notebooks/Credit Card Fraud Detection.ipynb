{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a44d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a6b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2817b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d420a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddc233",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec41768",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f85fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('D:/Projects/Credit Card Fraud Detection/creditcard.csv/creditcard.csv')\n",
    "print(df[df['V11'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Connection (Windows auth)\n",
    "engine = create_engine(\n",
    "    'mssql+pyodbc://DESKTOP-09SLK3B\\SQLEXPRESS/CreditCardData?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes'\n",
    ")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM dbo.creditcard\", engine)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88919194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Connect to SQL Server\n",
    "engine = create_engine(\n",
    "    'mssql+pyodbc://DESKTOP-09SLK3B\\SQLEXPRESS/CreditCardData?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes'\n",
    ")\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_sql(\"SELECT * FROM dbo.creditcard\", engine)\n",
    "\n",
    "# Validation Functions\n",
    "def check_row_count(df, expected=284807):\n",
    "    assert len(df) == expected, f\"Row count mismatch: {len(df)} != {expected}\"\n",
    "    print(f\"✅ Row count OK: {len(df)} rows\")\n",
    "\n",
    "def check_fraud_count(df, expected=492):\n",
    "    frauds = df['Class'].sum()\n",
    "    assert frauds == expected, f\"Fraud count mismatch: {frauds} != {expected}\"\n",
    "    print(f\"✅ Fraud count OK: {frauds} fraud cases\")\n",
    "\n",
    "def check_no_nans(df):\n",
    "    nulls = df.isnull().sum().sum()\n",
    "    assert nulls == 0, f\"Found {nulls} NaN values\"\n",
    "    print(\"✅ No missing values in dataset\")\n",
    "\n",
    "def check_ranges(df):\n",
    "    assert (df['Amount'] >= 0).all(), \"Negative amounts found\"\n",
    "    assert set(df['Class'].unique()).issubset({0,1}), \"Invalid Class values detected\"\n",
    "    print(\"✅ Value ranges OK (Amount >= 0, Class in {0,1})\")\n",
    "\n",
    "def check_duplicates(df):\n",
    "    dups = df.duplicated().sum()\n",
    "    print(f\"ℹ️ Duplicate rows: {dups}\")\n",
    "\n",
    "# Run Checks\n",
    "check_row_count(df)\n",
    "check_fraud_count(df)\n",
    "check_no_nans(df)\n",
    "check_ranges(df)\n",
    "check_duplicates(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "after = len(df)\n",
    "print(f\"Removed {before - after} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178ad9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Create folder for plots\n",
    "os.makedirs(\"sanity_plots\", exist_ok=True)\n",
    "\n",
    "# Fraud class distribution\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.countplot(x='Class', hue='Class', data=df, palette='Set2', legend=False)\n",
    "plt.title('Fraud Class Distribution')\n",
    "plt.xlabel('Class (0 = Legit, 1 = Fraud)')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(\"sanity_plots/fraud_class_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Transaction amount distribution (log scale)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(df['Amount'], bins=50, kde=False, color='blue')\n",
    "plt.yscale('log')\n",
    "plt.title('Transaction Amount Distribution (Log Scale)')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.savefig(\"sanity_plots/transaction_amount_distribution_log.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Time vs Amount scatter plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(df['Time'], df['Amount'], alpha=0.3, s=10)\n",
    "plt.title('Time vs Transaction Amount')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Amount')\n",
    "plt.savefig(\"sanity_plots/time_vs_amount.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap (first 10 features + Class)\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr = df.iloc[:, :10].join(df['Class']).corr()\n",
    "sns.heatmap(corr, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap (First 10 Features + Class)')\n",
    "plt.savefig(\"sanity_plots/correlation_heatmap_first10.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Sanity plots saved in /sanity_plots folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Create output folder\n",
    "output_dir = \"eda_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Class balance bar chart\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='Class', hue='Class', data=df, palette='Set2', legend=False)\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xlabel(\"Class (0 = Non-fraud, 1 = Fraud)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"class_distribution.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Amount histogram (log scale)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df['Amount'].apply(np.log1p), bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Transaction Amount Distribution (log scale)\")\n",
    "plt.xlabel(\"Log(Amount + 1)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"amount_distribution_log.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Time vs Class scatter\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.kdeplot(data=df, x=\"Time\", hue=\"Class\", fill=True, common_norm=False, alpha=0.5)\n",
    "plt.title(\"Transaction Time Density by Class\")\n",
    "plt.xlabel(\"Time (seconds since first transaction)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"time_vs_class_density.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap for V1–V28\n",
    "plt.figure(figsize=(12,10))\n",
    "corr = df.drop(columns=[\"Time\", \"Amount\", \"Class\"]).corr()\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Heatmap (V1–V28)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"correlation_heatmap.png\"))\n",
    "plt.show()\n",
    "\n",
    "#print(f\"Baseline PR-AUC (predict all non-fraud): {ap_baseline:.4f}\")\n",
    "#print(f\"Fraud ratio in data: {df['Class'].mean():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR-AUC Baseline (predict all zeros and random model)\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "y_true = df['Class'].values\n",
    "\n",
    "# Predict all zeros (all non-fraud)\n",
    "y_pred_all_zero = np.zeros_like(y_true)\n",
    "ap_baseline = average_precision_score(y_true, y_pred_all_zero)\n",
    "print(f\"Baseline PR-AUC (predict all non-fraud): {ap_baseline:.4f}\")\n",
    "\n",
    "# Trivial random model (predict random probabilities)\n",
    "np.random.seed(42)\n",
    "y_pred_random = np.random.uniform(0, 1, size=len(y_true))\n",
    "ap_random = average_precision_score(y_true, y_pred_random)\n",
    "print(f\"Random model PR-AUC: {ap_random:.4f}\")\n",
    "\n",
    "print(f\"Fraud ratio in data: {df['Class'].mean():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac3d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"creditcard_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6894a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get cut-off point around 1 day (~ 24h)\n",
    "t_cut = df[\"Time\"].quantile(0.5)   # median splits ~half-half (Day 1 vs Day 2)\n",
    "\n",
    "train = df[df[\"Time\"] < t_cut].copy()\n",
    "test  = df[df[\"Time\"] >= t_cut].copy()\n",
    "\n",
    "print(f\"Time-based split:\")\n",
    "print(f\"Train size: {len(train)} ({train['Class'].sum()} fraud cases)\")\n",
    "print(f\"Test size: {len(test)} ({test['Class'].sum()} fraud cases)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_curve, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Preprocessing Factory + Pipeline Constructor\n",
    "def make_column_transformer(X):\n",
    "    \"\"\"Factory for preprocessing transformer\"\"\"\n",
    "    num_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    return ColumnTransformer(\n",
    "        transformers=[(\"num\", StandardScaler(), num_features)],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "def make_pipeline(model, X):\n",
    "    \"\"\"Full pipeline with preprocessing + model\"\"\"\n",
    "    return Pipeline([\n",
    "        (\"preprocess\", make_column_transformer(X)),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "# Modeling + Evaluation\n",
    "results = {}\n",
    "pr_curves = {}\n",
    "roc_curves = {}\n",
    "\n",
    "def evaluate_model(name, y_true, y_pred, y_scores):\n",
    "    \"\"\"Print + store results\"\"\"\n",
    "    # Precision-Recall\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"PR-AUC: {pr_auc:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    # Save metrics\n",
    "    results[name] = {\n",
    "        \"PR-AUC\": pr_auc,\n",
    "        \"ROC-AUC\": roc_auc,\n",
    "        \"Precision\": classification_report(y_true, y_pred, output_dict=True)[\"1\"][\"precision\"],\n",
    "        \"Recall\": classification_report(y_true, y_pred, output_dict=True)[\"1\"][\"recall\"],\n",
    "        \"F1\": classification_report(y_true, y_pred, output_dict=True)[\"1\"][\"f1-score\"]\n",
    "    }\n",
    "    pr_curves[name] = (precision, recall)\n",
    "    roc_curves[name] = (fpr, tpr)\n",
    "\n",
    "# Supervised Comparable Pipelines\n",
    "supervised_models = {\n",
    "    \"LogReg_balanced\": LogisticRegression(class_weight=\"balanced\", max_iter=1000),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=10, class_weight=\"balanced\", n_jobs=-1, random_state=42\n",
    "    ),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(\n",
    "        n_estimators=300, num_leaves=32, learning_rate=0.05,\n",
    "        class_weight=\"balanced\", n_jobs=-1, random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "for name, model in supervised_models.items():\n",
    "    pipe = make_pipeline(model, train.drop(columns=[\"Class\"]))\n",
    "    pipe.fit(train.drop(columns=[\"Class\"]), train[\"Class\"])\n",
    "    y_pred = pipe.predict(test.drop(columns=[\"Class\"]))\n",
    "    y_scores = pipe.predict_proba(test.drop(columns=[\"Class\"]))[:, 1]\n",
    "    evaluate_model(name, test[\"Class\"], y_pred, y_scores)\n",
    "\n",
    "# Anomaly Detection Baselines\n",
    "anomaly_models = {\n",
    "    \"IsolationForest\": IsolationForest(contamination=0.001, random_state=42),\n",
    "    \"OneClassSVM\": OneClassSVM(kernel=\"rbf\", nu=0.001, gamma=\"scale\")\n",
    "}\n",
    "\n",
    "for name, model in anomaly_models.items():\n",
    "    model.fit(train.drop(columns=[\"Class\"]))\n",
    "    pred_labels = model.predict(test.drop(columns=[\"Class\"]))\n",
    "    y_pred = np.where(pred_labels == -1, 1, 0)\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        y_scores = -model.decision_function(test.drop(columns=[\"Class\"]))\n",
    "    else:\n",
    "        y_scores = -model.score_samples(test.drop(columns=[\"Class\"]))\n",
    "    evaluate_model(name, test[\"Class\"], y_pred, y_scores)\n",
    "\n",
    "# Final Summary Table\n",
    "summary_df = pd.DataFrame(results).T.round(4)\n",
    "print(\"\\n=== Summary of All Models ===\")\n",
    "print(summary_df)\n",
    "\n",
    "# PR Curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name, (precision, recall) in pr_curves.items():\n",
    "    plt.plot(recall, precision, label=f\"{name} (PR-AUC={results[name]['PR-AUC']:.3f})\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curves (All Models)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"pr_curves.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name, (fpr, tpr) in roc_curves.items():\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (ROC-AUC={results[name]['ROC-AUC']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves (All Models)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"roc_curves.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
